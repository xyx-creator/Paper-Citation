{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541b2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 1: imports (OGBN-Arxiv + GAT)\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyG / OGB\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "\n",
    "# 常用工具\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 数据处理 / 可视化 / 其他\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# typing / debug\n",
    "from typing import Optional, Tuple, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fec0572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded original OGB node features: torch.Size([169343, 128])\n",
      "==============================================================\n",
      "Nodes: 169343\n",
      "Edges: 2484941\n",
      "Feature dim (OGB original): 128\n",
      "Classes: 40\n",
      "Years: 1971 - 2020\n",
      "Train/Valid/Test: 90941/29799/48603\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "# block 2 (original features): Load OGBN-Arxiv graph using ORIGINAL OGB embeddings\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_self_loops, to_undirected\n",
    "\n",
    "# === 路径设置 ===\n",
    "base_dir = \"/home/xyx/gnn_demo/arxiv/ogbn_arxiv\"\n",
    "raw_dir = f\"{base_dir}/raw\"\n",
    "split_dir = f\"{base_dir}/split/time\"\n",
    "\n",
    "# === 1. 加载原始节点特征 ===\n",
    "node_feat = pd.read_csv(f\"{raw_dir}/node-feat.csv.gz\", compression=\"gzip\", header=None).values\n",
    "x = torch.tensor(node_feat, dtype=torch.float)\n",
    "print(f\"✅ Loaded original OGB node features: {x.shape}\")\n",
    "\n",
    "# === 2. 加载边关系、标签、年份 ===\n",
    "edge_index = pd.read_csv(f\"{raw_dir}/edge.csv.gz\", compression=\"gzip\", header=None).values.T\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "labels = pd.read_csv(f\"{raw_dir}/node-label.csv.gz\", compression=\"gzip\", header=None).values.squeeze()\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "years = pd.read_csv(f\"{raw_dir}/node_year.csv.gz\", compression=\"gzip\", header=None).values.squeeze()\n",
    "years = torch.tensor(years, dtype=torch.long)\n",
    "\n",
    "# === 3. 无向化 + 自环 ===\n",
    "edge_index = to_undirected(edge_index, num_nodes=x.shape[0])\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=x.shape[0])\n",
    "\n",
    "# === 4. 构造 PyG 数据对象 ===\n",
    "data = Data(x=x, edge_index=edge_index, y=labels)\n",
    "data.node_year = years\n",
    "\n",
    "# === 5. 加载训练/验证/测试划分 ===\n",
    "train_idx = torch.tensor(pd.read_csv(f\"{split_dir}/train.csv.gz\", compression=\"gzip\", header=None)[0].values)\n",
    "valid_idx = torch.tensor(pd.read_csv(f\"{split_dir}/valid.csv.gz\", compression=\"gzip\", header=None)[0].values)\n",
    "test_idx = torch.tensor(pd.read_csv(f\"{split_dir}/test.csv.gz\", compression=\"gzip\", header=None)[0].values)\n",
    "\n",
    "# === 6. 打印信息 ===\n",
    "print(\"==============================================================\")\n",
    "print(f\"Nodes: {data.num_nodes}\")\n",
    "print(f\"Edges: {data.num_edges}\")\n",
    "print(f\"Feature dim (OGB original): {data.num_features}\")\n",
    "print(f\"Classes: {int(labels.max()) + 1}\")\n",
    "print(f\"Years: {years.min().item()} - {years.max().item()}\")\n",
    "print(f\"Train/Valid/Test: {len(train_idx)}/{len(valid_idx)}/{len(test_idx)}\")\n",
    "print(\"==============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad09c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "in_dim=128, hidden_dim=128, out_dim=40\n",
      "\n",
      "--- GAT Model Architecture ---\n",
      "GAT(\n",
      "  (gat1): GATConv(128, 128, heads=4)\n",
      "  (gat2): GATConv(512, 128, heads=4)\n",
      "  (gat3): GATConv(512, 40, heads=1)\n",
      ")\n",
      "Epoch 010 | Loss: 2.4985 | Train: 0.4005 | Val: 0.4211 | Test: 0.4204\n",
      "Epoch 020 | Loss: 1.9429 | Train: 0.5496 | Val: 0.5672 | Test: 0.5528\n",
      "Epoch 030 | Loss: 1.7017 | Train: 0.6070 | Val: 0.6232 | Test: 0.6166\n",
      "Epoch 040 | Loss: 1.5717 | Train: 0.6323 | Val: 0.6410 | Test: 0.6341\n",
      "Epoch 050 | Loss: 1.5013 | Train: 0.6434 | Val: 0.6487 | Test: 0.6397\n",
      "Epoch 060 | Loss: 1.4601 | Train: 0.6542 | Val: 0.6600 | Test: 0.6540\n",
      "Epoch 070 | Loss: 1.4332 | Train: 0.6607 | Val: 0.6652 | Test: 0.6627\n",
      "Epoch 080 | Loss: 1.4089 | Train: 0.6675 | Val: 0.6695 | Test: 0.6633\n",
      "Epoch 090 | Loss: 1.3960 | Train: 0.6704 | Val: 0.6721 | Test: 0.6681\n",
      "Epoch 100 | Loss: 1.3916 | Train: 0.6738 | Val: 0.6733 | Test: 0.6659\n",
      "\n",
      "✅ Training finished! Best Val ACC: 0.6733, Corresponding Test ACC: 0.6672\n",
      "\n",
      "✅ Training logs (epochs_list, train_acc_list, val_acc_list, test_acc_list) are available in memory.\n"
     ]
    }
   ],
   "source": [
    "# block 3\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from ogb.nodeproppred import Evaluator\n",
    "\n",
    "# === 构建更复杂的 GAT ===\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.gat3 = GATConv(hidden_dim * heads, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gat3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# === 初始化设备与数据 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data = data.to(device)\n",
    "edge_index = to_undirected(data.edge_index, num_nodes=data.num_nodes)\n",
    "data.edge_index = edge_index\n",
    "\n",
    "# === 维度参数 ===\n",
    "in_dim = data.num_features         # 768 (SciBERT)\n",
    "hidden_dim = 128                   # 可调\n",
    "out_dim = int(data.y.max()) + 1    # 40 类\n",
    "print(f\"in_dim={in_dim}, hidden_dim={hidden_dim}, out_dim={out_dim}\")\n",
    "\n",
    "gat_model = GAT(in_dim, hidden_dim, out_dim, heads=4, dropout=0.4).to(device)\n",
    "print(\"\\n--- GAT Model Architecture ---\")\n",
    "print(gat_model)\n",
    "\n",
    "# === 优化器与损失 ===\n",
    "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.002, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# === 评估器 ===\n",
    "evaluator = Evaluator(name=\"ogbn-arxiv\")\n",
    "\n",
    "# === 训练与验证循环 ===\n",
    "def train():\n",
    "    gat_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = gat_model(data.x, data.edge_index)\n",
    "    loss = criterion(out[train_idx], data.y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    gat_model.eval()\n",
    "    out = gat_model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[train_idx].unsqueeze(-1),\n",
    "        'y_pred': y_pred[train_idx]\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[valid_idx].unsqueeze(-1),\n",
    "        'y_pred': y_pred[valid_idx]\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[test_idx].unsqueeze(-1),\n",
    "        'y_pred': y_pred[test_idx]\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "# === 主训练过程 ===\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "# ✅ 新增：仅保存在内存中（用于后续 block 4）\n",
    "epochs_list, train_acc_list, val_acc_list, test_acc_list = [], [], [], []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "\n",
    "    # 记录每个 epoch 的数据\n",
    "    epochs_list.append(epoch)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Training finished! Best Val ACC: {best_val_acc:.4f}, \"\n",
    "      f\"Corresponding Test ACC: {best_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Training logs (epochs_list, train_acc_list, val_acc_list, test_acc_list) are available in memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4234c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # block 5 (OGB original – NO_LEAK + optional LOW_DATA)\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "# import torch_geometric.utils as utils\n",
    "# import time\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # === 参数区 ===\n",
    "# LOW_DATA_MODE = True   # ← 改为 False 可使用全部训练边\n",
    "# LOW_DATA_RATIO = 0.05  # 仅当 LOW_DATA_MODE=True 时生效 (可改 0.01)\n",
    "\n",
    "# print(\"\\n--- [No-Leak + Low-Data] OGBN-Arxiv Link Prediction Prep (OGB Original) ---\")\n",
    "# start_time = time.time()\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# gat_model.eval()\n",
    "# data = data.to(device)\n",
    "\n",
    "# # === Step 1: 边划分 ===\n",
    "# print(\"[1/6] Splitting edges (10% val / 10% test)...\")\n",
    "# edge_index_full = data.edge_index.cpu()\n",
    "# lp_data = Data(x=data.x.cpu(), edge_index=edge_index_full)\n",
    "# split_data_arxiv = utils.train_test_split_edges(lp_data, val_ratio=0.1, test_ratio=0.1)\n",
    "# print(\"✅ Edge split complete.\")\n",
    "\n",
    "# # === Step 2: 可选 Low-Data 下采样 ===\n",
    "# if LOW_DATA_MODE:\n",
    "#     ratio = LOW_DATA_RATIO\n",
    "#     num_total = split_data_arxiv.train_pos_edge_index.size(1)\n",
    "#     num_keep = int(num_total * ratio)\n",
    "#     perm = torch.randperm(num_total)[:num_keep]\n",
    "#     split_data_arxiv.train_pos_edge_index = split_data_arxiv.train_pos_edge_index[:, perm]\n",
    "#     print(f\"✅ Low-Data mode ON: Kept {num_keep} training edges ({ratio*100:.1f}%)\")\n",
    "# else:\n",
    "#     print(\"✅ Low-Data mode OFF: Using all training edges.\")\n",
    "\n",
    "# # === Step 3: 构建训练子图 ===\n",
    "# train_subgraph = Data(\n",
    "#     x=data.x.cpu(),\n",
    "#     edge_index=split_data_arxiv.train_pos_edge_index.cpu()\n",
    "# )\n",
    "# print(f\"✅ Train subgraph built with {train_subgraph.edge_index.shape[1]} edges.\")\n",
    "\n",
    "# # === Step 4: 重算 GAT 嵌入（仅训练边，避免信息泄漏） ===\n",
    "# print(\"[4/6] Recomputing GAT embeddings on train-only subgraph (OGB features)...\")\n",
    "# train_subgraph = train_subgraph.to(device)\n",
    "# with torch.no_grad():\n",
    "#     h = F.dropout(train_subgraph.x, p=gat_model.dropout, training=False)\n",
    "#     h = gat_model.gat1(h, train_subgraph.edge_index); h = F.elu(h)\n",
    "#     h = gat_model.gat2(h, train_subgraph.edge_index); h = F.elu(h)\n",
    "#     node_embeddings_arxiv = h.detach().cpu()\n",
    "# print(f\"✅ New embeddings computed: {node_embeddings_arxiv.shape}\")\n",
    "\n",
    "# # === Step 5: 负采样（1:5） ===\n",
    "# print(\"[5/6] Sampling negative edges (1:5 ratio)...\")\n",
    "# num_nodes = data.num_nodes\n",
    "# num_train_pos = split_data_arxiv.train_pos_edge_index.size(1)\n",
    "# num_train_neg = num_train_pos * 5\n",
    "# train_neg_edge_index = utils.negative_sampling(\n",
    "#     edge_index=split_data_arxiv.train_pos_edge_index,\n",
    "#     num_nodes=num_nodes,\n",
    "#     num_neg_samples=num_train_neg,\n",
    "#     method='sparse'\n",
    "# )\n",
    "# split_data_arxiv.train_neg_edge_index = train_neg_edge_index\n",
    "# print(f\"✅ Negative sampling done. Train +: {num_train_pos}, Train -: {num_train_neg}\")\n",
    "\n",
    "# # === Step 6: 移到 GPU ===\n",
    "# split_data_arxiv = split_data_arxiv.to(device)\n",
    "# node_embeddings_arxiv = node_embeddings_arxiv.to(device)\n",
    "\n",
    "# elapsed = time.time() - start_time\n",
    "# print(\"\\n--- Summary ---\")\n",
    "# print(f\"Nodes: {num_nodes}\")\n",
    "# print(f\"Train pos: {num_train_pos}\")\n",
    "# print(f\"Train neg: {num_train_neg}\")\n",
    "# print(f\"Val pos: {split_data_arxiv.val_pos_edge_index.shape[1]}\")\n",
    "# print(f\"Test pos: {split_data_arxiv.test_pos_edge_index.shape[1]}\")\n",
    "# print(f\"Runtime: {elapsed:.2f}s (~{elapsed/60:.2f} min)\")\n",
    "# print(\"✅ Block 5 (No-Leak + Low-Data, OGB original) finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa61729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # block 5.1 (save – OGB original)\n",
    "# torch.save({\n",
    "#     'node_embeddings_arxiv': node_embeddings_arxiv.cpu(),\n",
    "#     'split_data_arxiv': {\n",
    "#         'train_pos_edge_index': split_data_arxiv.train_pos_edge_index.cpu(),\n",
    "#         'train_neg_edge_index': split_data_arxiv.train_neg_edge_index.cpu(),\n",
    "#         'val_pos_edge_index': split_data_arxiv.val_pos_edge_index.cpu(),\n",
    "#         'val_neg_edge_index': split_data_arxiv.val_neg_edge_index.cpu(),\n",
    "#         'test_pos_edge_index': split_data_arxiv.test_pos_edge_index.cpu(),\n",
    "#         'test_neg_edge_index': split_data_arxiv.test_neg_edge_index.cpu(),\n",
    "#     }\n",
    "# }, \"ogbn_arxiv_link_data_ogb.pt\")\n",
    "\n",
    "# print(\"✅ Saved OGB-original embeddings and splits to ogbn_arxiv_link_data_ogb.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587772bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Block 5.2] Loading Pre-Saved ogbn_arxiv_link_data_ogb.pt ---\n",
      "✅ Loaded node embeddings: (169343, 512)\n",
      "✅ All components loaded and moved to device.\n",
      "Train pos edges: 92624\n",
      "Train neg edges: 463120\n",
      "Val pos edges:   115779\n",
      "Test pos edges:  115779\n",
      "Device: cuda\n",
      "\n",
      "--- Block 5.2 ready. You can proceed to Block 6 (LinkPredictor definition). ---\n"
     ]
    }
   ],
   "source": [
    "# block 5.2 (OGB original): Load Pre-Saved ogbn_arxiv_link_data_ogb.pt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "print(\"\\n--- [Block 5.2] Loading Pre-Saved ogbn_arxiv_link_data_ogb.pt ---\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Step 1: 加载保存文件 ===\n",
    "checkpoint = torch.load(\"ogbn_arxiv_link_data_ogb.pt\", map_location=\"cpu\")\n",
    "\n",
    "# === Step 2: 恢复节点嵌入 ===\n",
    "node_embeddings_arxiv = checkpoint[\"node_embeddings_arxiv\"]\n",
    "print(f\"✅ Loaded node embeddings: {tuple(node_embeddings_arxiv.shape)}\")\n",
    "\n",
    "# === Step 3: 恢复边划分数据 ===\n",
    "split_dict = checkpoint[\"split_data_arxiv\"]\n",
    "\n",
    "# 构造 Data 对象（与 Block 5 输出结构保持一致）\n",
    "split_data_arxiv = Data()\n",
    "split_data_arxiv.train_pos_edge_index = split_dict[\"train_pos_edge_index\"]\n",
    "split_data_arxiv.train_neg_edge_index = split_dict[\"train_neg_edge_index\"]\n",
    "split_data_arxiv.val_pos_edge_index   = split_dict[\"val_pos_edge_index\"]\n",
    "split_data_arxiv.val_neg_edge_index   = split_dict[\"val_neg_edge_index\"]\n",
    "split_data_arxiv.test_pos_edge_index  = split_dict[\"test_pos_edge_index\"]\n",
    "split_data_arxiv.test_neg_edge_index  = split_dict[\"test_neg_edge_index\"]\n",
    "\n",
    "# === Step 4: 移动到 GPU ===\n",
    "node_embeddings_arxiv = node_embeddings_arxiv.to(device)\n",
    "split_data_arxiv = split_data_arxiv.to(device)\n",
    "\n",
    "# === Step 5: 打印验证信息 ===\n",
    "print(\"✅ All components loaded and moved to device.\")\n",
    "print(f\"Train pos edges: {split_data_arxiv.train_pos_edge_index.shape[1]}\")\n",
    "print(f\"Train neg edges: {split_data_arxiv.train_neg_edge_index.shape[1]}\")\n",
    "print(f\"Val pos edges:   {split_data_arxiv.val_pos_edge_index.shape[1]}\")\n",
    "print(f\"Test pos edges:  {split_data_arxiv.test_pos_edge_index.shape[1]}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\n--- Block 5.2 ready. You can proceed to Block 6 (LinkPredictor definition). ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307d6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Block 5.3] Recomputing GAT embeddings using TRAIN edges only (OGB original) ---\n",
      "Using only train edges: 180516 edges for GAT propagation.\n",
      "✅ Done. New train-only embeddings computed: torch.Size([169343, 512])\n",
      "Runtime: 0.02s\n",
      "✅ node_embeddings_arxiv replaced with train-only embeddings.\n",
      "\n",
      "--- Block 5.3 ready. You can proceed to Block 6 (LinkPredictor definition). ---\n"
     ]
    }
   ],
   "source": [
    "# block 5.3 (OGB original): Recompute GAT embeddings (Train Edges Only, No Leakage)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected\n",
    "import time\n",
    "\n",
    "print(\"\\n--- [Block 5.3] Recomputing GAT embeddings using TRAIN edges only (OGB original) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gat_model = gat_model.to(device)\n",
    "gat_model.eval()\n",
    "\n",
    "# === Step 1: 构造仅包含训练正样本边的子图 ===\n",
    "train_edge_index = split_data_arxiv.train_pos_edge_index\n",
    "train_edge_index = to_undirected(train_edge_index)  # 确保为无向图\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "\n",
    "print(f\"Using only train edges: {train_edge_index.shape[1]} edges for GAT propagation.\")\n",
    "\n",
    "# === Step 2: 基于训练子图计算节点嵌入（防止信息泄漏） ===\n",
    "with torch.no_grad():\n",
    "    x = data.x.to(device)\n",
    "\n",
    "    h = F.dropout(x, p=gat_model.dropout, training=False)\n",
    "    h = gat_model.gat1(h, train_edge_index); h = F.elu(h)\n",
    "    h = gat_model.gat2(h, train_edge_index); h = F.elu(h)\n",
    "    node_embeddings_arxiv = h.detach()\n",
    "\n",
    "print(f\"✅ Done. New train-only embeddings computed: {node_embeddings_arxiv.shape}\")\n",
    "print(f\"Runtime: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# === Step 3: 移动到 GPU（用于后续 Block 6–9） ===\n",
    "node_embeddings_arxiv = node_embeddings_arxiv.to(device)\n",
    "print(\"✅ node_embeddings_arxiv replaced with train-only embeddings.\")\n",
    "print(\"\\n--- Block 5.3 ready. You can proceed to Block 6 (LinkPredictor definition). ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ac094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Block 5.4] Applying Optional Low-Data Sampling (OGB Original) ---\n",
      "✅ Low-Data mode ON: kept 926/92624 training edges (1.0%)\n",
      "[Resampling negative edges to match new ratio]...\n",
      "✅ Negative edges resampled: Train + 926, Train - 4630\n",
      "Runtime: 0.04s\n",
      "✅ Block 5.4 finished successfully. You can proceed to Block 6 (LinkPredictor definition).\n"
     ]
    }
   ],
   "source": [
    "# block 5.4 (OGB original): Apply Low-Data Sampling to Loaded Split\n",
    "import torch\n",
    "import time\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "print(\"\\n--- [Block 5.4] Applying Optional Low-Data Sampling (OGB Original) ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# === Step 0: 参数开关 ===\n",
    "LOW_DATA_MODE = True      # ← 改为 False 关闭下采样\n",
    "LOW_DATA_RATIO = 0.01     # 仅当 LOW_DATA_MODE=True 时生效 (可调 0.01, 0.05 等)\n",
    "\n",
    "# === Step 1: 确认 split_data_arxiv 是否存在 ===\n",
    "assert 'split_data_arxiv' in locals(), \"❌ split_data_arxiv not found. Run Block 5.2 or 5.3 first.\"\n",
    "\n",
    "if LOW_DATA_MODE:\n",
    "    num_total = split_data_arxiv.train_pos_edge_index.size(1)\n",
    "    num_keep = int(num_total * LOW_DATA_RATIO)\n",
    "    perm = torch.randperm(num_total)[:num_keep]\n",
    "    split_data_arxiv.train_pos_edge_index = split_data_arxiv.train_pos_edge_index[:, perm]\n",
    "    print(f\"✅ Low-Data mode ON: kept {num_keep}/{num_total} training edges ({LOW_DATA_RATIO*100:.1f}%)\")\n",
    "\n",
    "    # --- Step 2: 同步重新采样负边（保持 1:5 比例） ---\n",
    "    print(\"[Resampling negative edges to match new ratio]...\")\n",
    "    num_nodes = data.num_nodes\n",
    "    num_train_pos = split_data_arxiv.train_pos_edge_index.size(1)\n",
    "    num_train_neg = num_train_pos * 5\n",
    "\n",
    "    split_data_arxiv.train_neg_edge_index = negative_sampling(\n",
    "        edge_index=split_data_arxiv.train_pos_edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=num_train_neg,\n",
    "        method='sparse'\n",
    "    )\n",
    "    print(f\"✅ Negative edges resampled: Train + {num_train_pos}, Train - {num_train_neg}\")\n",
    "else:\n",
    "    print(\"✅ Low-Data mode OFF: keeping full training edge set.\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Runtime: {elapsed:.2f}s\")\n",
    "print(\"✅ Block 5.4 finished successfully. You can proceed to Block 6 (LinkPredictor definition).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc70646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced LinkPredictor defined (OGB original version: 3-layer MLP with dropout).\n"
     ]
    }
   ],
   "source": [
    "# block 6 (OGB original): Enhanced LinkPredictor definition\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, all_node_embeddings, edge_label_index):\n",
    "        # edge_label_index shape: [2, num_edges]\n",
    "        src = all_node_embeddings[edge_label_index[0]]\n",
    "        dst = all_node_embeddings[edge_label_index[1]]\n",
    "        h = torch.cat([src, dst], dim=1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        return self.fc3(h).squeeze(-1)\n",
    "\n",
    "print(\"✅ Enhanced LinkPredictor defined (OGB original version: 3-layer MLP with dropout).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cb3a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ LinkPredictor model for OGBN-Arxiv (OGB Original features) initialized on cuda.\n",
      "Embedding dimension: 512\n",
      "LinkPredictor(\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# block 7 (OGB original): Initialize LinkPredictor, optimizer, and loss\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 自动推断嵌入维度 ===\n",
    "embedding_dim_arxiv = node_embeddings_arxiv.shape[1]\n",
    "\n",
    "# === 初始化链路预测器 ===\n",
    "link_predictor_arxiv = LinkPredictor(\n",
    "    embedding_dim=embedding_dim_arxiv,\n",
    "    hidden_dim=128,     # 可调，例如 128 / 256\n",
    "    dropout=0.3         # 稍微强一点的正则化\n",
    ").to(device)\n",
    "\n",
    "# === 优化器 ===\n",
    "optimizer_link_arxiv = optim.AdamW(\n",
    "    link_predictor_arxiv.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# === 损失函数 ===\n",
    "criterion_link_arxiv = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"\\n✅ LinkPredictor model for OGBN-Arxiv (OGB Original features) initialized on {device}.\")\n",
    "print(f\"Embedding dimension: {embedding_dim_arxiv}\")\n",
    "print(link_predictor_arxiv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab50224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train_link_pred_arxiv() and test_link_pred_arxiv() defined (OGB Original version).\n"
     ]
    }
   ],
   "source": [
    "# block 8 (OGB original): Train and test functions for link prediction\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_link_pred_arxiv(pos_edge_index, neg_edge_index, batch_size=1024):\n",
    "    \"\"\"\n",
    "    评估链路预测模型在给定边集上的 AUC。\n",
    "    使用 batch 计算避免显存溢出。\n",
    "    \"\"\"\n",
    "    link_predictor_arxiv.eval()\n",
    "    pos_scores, neg_scores = [], []\n",
    "    total_pos = pos_edge_index.size(1)\n",
    "    total_neg = neg_edge_index.size(1)\n",
    "\n",
    "    # === 正样本批次计算 ===\n",
    "    for start in range(0, total_pos, batch_size):\n",
    "        end = min(start + batch_size, total_pos)\n",
    "        edges = pos_edge_index[:, start:end]\n",
    "        logits = link_predictor_arxiv(node_embeddings_arxiv, edges)\n",
    "        pos_scores.append(logits.sigmoid().cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # === 负样本批次计算 ===\n",
    "    for start in range(0, total_neg, batch_size):\n",
    "        end = min(start + batch_size, total_neg)\n",
    "        edges = neg_edge_index[:, start:end]\n",
    "        logits = link_predictor_arxiv(node_embeddings_arxiv, edges)\n",
    "        neg_scores.append(logits.sigmoid().cpu())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # === 拼接结果 ===\n",
    "    pos_scores = torch.cat(pos_scores)\n",
    "    neg_scores = torch.cat(neg_scores)\n",
    "    scores = torch.cat([pos_scores, neg_scores])\n",
    "    labels = torch.cat([\n",
    "        torch.ones(pos_scores.size(0)),\n",
    "        torch.zeros(neg_scores.size(0))\n",
    "    ])\n",
    "\n",
    "    auc = roc_auc_score(labels.numpy(), scores.numpy())\n",
    "    return auc\n",
    "\n",
    "\n",
    "def train_link_pred_arxiv(batch_size=128):\n",
    "    \"\"\"\n",
    "    在训练集上优化链路预测器。\n",
    "    随机混合正负样本并以 batch 训练。\n",
    "    \"\"\"\n",
    "    link_predictor_arxiv.train()\n",
    "    optimizer_link_arxiv.zero_grad()\n",
    "\n",
    "    pos_edges = split_data_arxiv.train_pos_edge_index\n",
    "    neg_edges = split_data_arxiv.train_neg_edge_index\n",
    "    num_pos = pos_edges.size(1)\n",
    "    num_neg = neg_edges.size(1)\n",
    "\n",
    "    # === 合并正负样本 ===\n",
    "    all_edges = torch.cat([pos_edges, neg_edges], dim=1)\n",
    "    all_labels = torch.cat([\n",
    "        torch.ones(num_pos, device=device),\n",
    "        torch.zeros(num_neg, device=device)\n",
    "    ])\n",
    "    perm = torch.randperm(num_pos + num_neg, device=device)\n",
    "    all_edges, all_labels = all_edges[:, perm], all_labels[perm]\n",
    "\n",
    "    total_loss, steps = 0.0, 0\n",
    "    for start in range(0, all_edges.size(1), batch_size):\n",
    "        end = min(start + batch_size, all_edges.size(1))\n",
    "        e = all_edges[:, start:end]\n",
    "        lbl = all_labels[start:end]\n",
    "\n",
    "        logits = link_predictor_arxiv(node_embeddings_arxiv, e)\n",
    "        loss = criterion_link_arxiv(logits, lbl)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_link_arxiv.step()\n",
    "        optimizer_link_arxiv.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * (end - start)\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 1000 == 0:\n",
    "            print(f\"Batch {steps}: avg_loss={total_loss/(steps*batch_size):.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / all_edges.size(1)\n",
    "\n",
    "print(\"✅ train_link_pred_arxiv() and test_link_pred_arxiv() defined (OGB Original version).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4911de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting OGBN-Arxiv Link Predictor Training (OGB Original features) ---\n",
      "Epoch 001 | Loss: 0.3896 | Train AUC: 0.9617 | Val AUC: 0.8689 | Test AUC: 0.8699 | Time: 0.01 min\n",
      "Epoch 005 | Loss: 0.0822 | Train AUC: 0.9966 | Val AUC: 0.9091 | Test AUC: 0.9093 | Time: 0.03 min\n",
      "Epoch 010 | Loss: 0.0550 | Train AUC: 0.9981 | Val AUC: 0.9081 | Test AUC: 0.9077 | Time: 0.04 min\n",
      "\n",
      "--- Training Finished (OGB Original features) ---\n",
      "✅ Best Validation AUC: 0.9104\n",
      "Best model state reloaded.\n",
      "Final Test AUC (OGB Original): 0.9097\n"
     ]
    }
   ],
   "source": [
    "# block 9 (OGB original): Train loop for OGBN-Arxiv link predictor\n",
    "import copy, time\n",
    "\n",
    "best_val_auc_arxiv = 0.0\n",
    "best_model_state_link_arxiv = None\n",
    "num_epochs_link_arxiv = 10  # 可根据需要调大，例如 30 或 50\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n--- Starting OGBN-Arxiv Link Predictor Training (OGB Original features) ---\")\n",
    "\n",
    "for epoch in range(1, num_epochs_link_arxiv + 1):\n",
    "    # === 训练一个 epoch ===\n",
    "    loss_arxiv = train_link_pred_arxiv()\n",
    "    \n",
    "    # === 验证集 AUC ===\n",
    "    val_auc_arxiv = test_link_pred_arxiv(\n",
    "        split_data_arxiv.val_pos_edge_index,\n",
    "        split_data_arxiv.val_neg_edge_index\n",
    "    )\n",
    "\n",
    "    # === 保存最佳模型 ===\n",
    "    if val_auc_arxiv > best_val_auc_arxiv:\n",
    "        best_val_auc_arxiv = val_auc_arxiv\n",
    "        best_model_state_link_arxiv = copy.deepcopy(link_predictor_arxiv.state_dict())\n",
    "\n",
    "    # === 每隔若干 epoch 打印训练 / 测试结果 ===\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        train_auc = test_link_pred_arxiv(\n",
    "            split_data_arxiv.train_pos_edge_index,\n",
    "            split_data_arxiv.train_neg_edge_index\n",
    "        )\n",
    "        test_auc = test_link_pred_arxiv(\n",
    "            split_data_arxiv.test_pos_edge_index,\n",
    "            split_data_arxiv.test_neg_edge_index\n",
    "        )\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"Loss: {loss_arxiv:.4f} | \"\n",
    "              f\"Train AUC: {train_auc:.4f} | \"\n",
    "              f\"Val AUC: {val_auc_arxiv:.4f} | \"\n",
    "              f\"Test AUC: {test_auc:.4f} | \"\n",
    "              f\"Time: {elapsed/60:.2f} min\")\n",
    "\n",
    "print(\"\\n--- Training Finished (OGB Original features) ---\")\n",
    "print(f\"✅ Best Validation AUC: {best_val_auc_arxiv:.4f}\")\n",
    "\n",
    "# === 加载最佳模型 ===\n",
    "if best_model_state_link_arxiv:\n",
    "    link_predictor_arxiv.load_state_dict(best_model_state_link_arxiv)\n",
    "    print(\"Best model state reloaded.\")\n",
    "\n",
    "# === 最终测试集评估 ===\n",
    "final_test_auc_arxiv = test_link_pred_arxiv(\n",
    "    split_data_arxiv.test_pos_edge_index,\n",
    "    split_data_arxiv.test_neg_edge_index\n",
    ")\n",
    "print(f\"Final Test AUC (OGB Original): {final_test_auc_arxiv:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7561f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Ablation A1-OGB] No-GAT (Original Embedding) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A1-OGB No-GAT] Epoch 010 | Loss: 2.8105 | Train: 0.2688 | Val: 0.2728 | Test: 0.2413\n",
      "[A1-OGB No-GAT] Epoch 020 | Loss: 2.4163 | Train: 0.3901 | Val: 0.4068 | Test: 0.3850\n",
      "[A1-OGB No-GAT] Epoch 030 | Loss: 2.2498 | Train: 0.4358 | Val: 0.4588 | Test: 0.4431\n",
      "[A1-OGB No-GAT] Epoch 040 | Loss: 2.1555 | Train: 0.4671 | Val: 0.4807 | Test: 0.4606\n",
      "[A1-OGB No-GAT] Epoch 050 | Loss: 2.1026 | Train: 0.4786 | Val: 0.4926 | Test: 0.4752\n",
      "[A1-OGB No-GAT] Epoch 060 | Loss: 2.0715 | Train: 0.4847 | Val: 0.4985 | Test: 0.4801\n",
      "[A1-OGB No-GAT] Epoch 070 | Loss: 2.0577 | Train: 0.4889 | Val: 0.4999 | Test: 0.4822\n",
      "[A1-OGB No-GAT] Epoch 080 | Loss: 2.0447 | Train: 0.4916 | Val: 0.5021 | Test: 0.4837\n",
      "[A1-OGB No-GAT] Epoch 090 | Loss: 2.0381 | Train: 0.4940 | Val: 0.5048 | Test: 0.4854\n",
      "[A1-OGB No-GAT] Epoch 100 | Loss: 2.0282 | Train: 0.4962 | Val: 0.5073 | Test: 0.4889\n",
      "\n",
      "✅ [A1-OGB No-GAT] Finished. Best Val ACC: 0.5073, Corresponding Test ACC: 0.4889\n"
     ]
    }
   ],
   "source": [
    "# block A1-OGB (Ablation: No GAT, Same Head, Original OGB Embedding)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator\n",
    "\n",
    "print(\"\\n--- [Ablation A1-OGB] No-GAT (Original Embedding) ---\")\n",
    "\n",
    "# === 模型定义 ===\n",
    "class NoGAT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_dim, hidden_dim * 4)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim * 4, hidden_dim * 4)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim * 4, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index=None):  # 保留接口，但不使用图结构\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# === 初始化 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)   # 此时 data.x 是 OGB 原生 embedding（dim=128）\n",
    "\n",
    "in_dim = data.num_features       # 128\n",
    "hidden_dim = 128\n",
    "out_dim = int(data.y.max()) + 1\n",
    "\n",
    "nogat_model_ogb = NoGAT(in_dim, hidden_dim, out_dim, dropout=0.4).to(device)\n",
    "optimizer = torch.optim.Adam(nogat_model_ogb.parameters(), lr=0.002, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "evaluator = Evaluator(name=\"ogbn-arxiv\")\n",
    "\n",
    "# === 训练函数 ===\n",
    "def train():\n",
    "    nogat_model_ogb.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = nogat_model_ogb(data.x)  # 不使用边\n",
    "    loss = criterion(out[train_idx], data.y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# === 测试函数 ===\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    nogat_model_ogb.eval()\n",
    "    out = nogat_model_ogb(data.x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    train_acc = evaluator.eval({'y_true': data.y[train_idx].unsqueeze(-1), 'y_pred': y_pred[train_idx]})['acc']\n",
    "    val_acc = evaluator.eval({'y_true': data.y[valid_idx].unsqueeze(-1), 'y_pred': y_pred[valid_idx]})['acc']\n",
    "    test_acc = evaluator.eval({'y_true': data.y[test_idx].unsqueeze(-1), 'y_pred': y_pred[test_idx]})['acc']\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "# === 主训练循环 ===\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[A1-OGB No-GAT] Epoch {epoch:03d} | Loss: {loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [A1-OGB No-GAT] Finished. Best Val ACC: {best_val_acc:.4f}, \"\n",
    "      f\"Corresponding Test ACC: {best_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e104f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [A2-OGB] Param-Matched No-GAT (Original OGB Embedding) ---\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 010 | Loss: 2.9076 | Train: 0.2422 | Val: 0.2126 | Test: 0.1854\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 020 | Loss: 2.5290 | Train: 0.3601 | Val: 0.3794 | Test: 0.3583\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 030 | Loss: 2.3498 | Train: 0.4065 | Val: 0.4294 | Test: 0.4125\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 040 | Loss: 2.2247 | Train: 0.4430 | Val: 0.4591 | Test: 0.4448\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 050 | Loss: 2.1582 | Train: 0.4637 | Val: 0.4787 | Test: 0.4601\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 060 | Loss: 2.1153 | Train: 0.4752 | Val: 0.4893 | Test: 0.4709\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 070 | Loss: 2.0904 | Train: 0.4805 | Val: 0.4938 | Test: 0.4754\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 080 | Loss: 2.0755 | Train: 0.4851 | Val: 0.4969 | Test: 0.4785\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 090 | Loss: 2.0621 | Train: 0.4888 | Val: 0.5010 | Test: 0.4802\n",
      "[A2-OGB No-GAT ParamMatched] Epoch 100 | Loss: 2.0522 | Train: 0.4905 | Val: 0.5019 | Test: 0.4821\n",
      "\n",
      "✅ [A2-OGB No-GAT ParamMatched] Finished. Best Val ACC: 0.5025, Test ACC: 0.4820\n"
     ]
    }
   ],
   "source": [
    "# block A2-OGB (Ablation: Param-Matched No-GAT, Original OGB Embedding)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import Evaluator\n",
    "\n",
    "print(\"\\n--- [A2-OGB] Param-Matched No-GAT (Original OGB Embedding) ---\")\n",
    "\n",
    "# === 模型定义 ===\n",
    "class NoGAT_ParamMatched(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.4):\n",
    "        super().__init__()\n",
    "        # 这里 hidden_dim 仍然调大，用于匹配 GAT 的参数规模\n",
    "        self.fc1 = torch.nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index=None):  # 不使用 edge_index\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# === 初始化 ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)  # 此时 data.x 是 OGB 原始 embedding (128维)\n",
    "\n",
    "in_dim = data.num_features       # 128\n",
    "hidden_dim = 384                 # 调大，匹配 GAT 的多头参数数量\n",
    "out_dim = int(data.y.max()) + 1\n",
    "\n",
    "nogat_matched_ogb = NoGAT_ParamMatched(in_dim, hidden_dim, out_dim, dropout=0.4).to(device)\n",
    "optimizer = torch.optim.Adam(nogat_matched_ogb.parameters(), lr=0.002, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "evaluator = Evaluator(name=\"ogbn-arxiv\")\n",
    "\n",
    "# === 训练函数 ===\n",
    "def train():\n",
    "    nogat_matched_ogb.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = nogat_matched_ogb(data.x)\n",
    "    loss = criterion(out[train_idx], data.y[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# === 测试函数 ===\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    nogat_matched_ogb.eval()\n",
    "    out = nogat_matched_ogb(data.x)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "    train_acc = evaluator.eval({'y_true': data.y[train_idx].unsqueeze(-1), 'y_pred': y_pred[train_idx]})['acc']\n",
    "    val_acc = evaluator.eval({'y_true': data.y[valid_idx].unsqueeze(-1), 'y_pred': y_pred[valid_idx]})['acc']\n",
    "    test_acc = evaluator.eval({'y_true': data.y[test_idx].unsqueeze(-1), 'y_pred': y_pred[test_idx]})['acc']\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "# === 主训练循环 ===\n",
    "best_val_acc, best_test_acc = 0, 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc, best_test_acc = val_acc, test_acc\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"[A2-OGB No-GAT ParamMatched] Epoch {epoch:03d} | \"\n",
    "              f\"Loss: {loss:.4f} | Train: {train_acc:.4f} | \"\n",
    "              f\"Val: {val_acc:.4f} | Test: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [A2-OGB No-GAT ParamMatched] Finished. \"\n",
    "      f\"Best Val ACC: {best_val_acc:.4f}, Test ACC: {best_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b76a175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A3-OGB] Using original OGB node embeddings: (169343, 128)\n",
      "[A3-OGB][Epoch 01] loss=0.6272 | trainAUC=0.6146 | valAUC=0.6066 | testAUC=0.6084\n",
      "[A3-OGB][Epoch 05] loss=0.6046 | trainAUC=0.6667 | valAUC=0.6512 | testAUC=0.6529\n",
      "[A3-OGB][Epoch 10] loss=0.5800 | trainAUC=0.6782 | valAUC=0.6578 | testAUC=0.6594\n",
      "\n",
      "✅ [A3-OGB Ablation] Final Test AUC (OGB-only): 0.6594\n"
     ]
    }
   ],
   "source": [
    "# block A3-OGB (Ablation: OGB-only Link Prediction, no GAT)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 0) 检查依赖 ===\n",
    "assert 'data' in locals(), \"❌ data 未定义（应包含 OGB 原始特征 data.x）\"\n",
    "assert 'split_data_arxiv' in locals(), \"❌ 缺少 split_data_arxiv（请先运行 block 5.2 或 5.4）\"\n",
    "\n",
    "# === 1) 使用 OGB 原生节点特征（不经过 GAT） ===\n",
    "node_embeddings_ogb = data.x.to(device)   # [N, 128]\n",
    "embed_dim_ogb = node_embeddings_ogb.shape[1]\n",
    "print(f\"[A3-OGB] Using original OGB node embeddings: {tuple(node_embeddings_ogb.shape)}\")\n",
    "\n",
    "# === 2) 定义轻量链路预测器 ===\n",
    "class LinkPredictorOGB(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, all_node_embeddings, edge_label_index):\n",
    "        src = all_node_embeddings[edge_label_index[0]]\n",
    "        dst = all_node_embeddings[edge_label_index[1]]\n",
    "        h = torch.cat([src, dst], dim=1)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        return self.fc2(h).squeeze(-1)\n",
    "\n",
    "link_predictor_ogb = LinkPredictorOGB(embed_dim_ogb, hidden_dim=64).to(device)\n",
    "optimizer_ogb = torch.optim.AdamW(link_predictor_ogb.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion_ogb = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === 3) 训练函数 ===\n",
    "def train_link_pred_ogb(batch_size=512):\n",
    "    link_predictor_ogb.train()\n",
    "    optimizer_ogb.zero_grad()\n",
    "\n",
    "    pos_edges = split_data_arxiv.train_pos_edge_index\n",
    "    neg_edges = split_data_arxiv.train_neg_edge_index\n",
    "    num_pos, num_neg = pos_edges.size(1), neg_edges.size(1)\n",
    "\n",
    "    perm_pos = torch.randperm(num_pos, device=device)\n",
    "    perm_neg = torch.randperm(num_neg, device=device)\n",
    "\n",
    "    total_loss, steps = 0.0, 0\n",
    "    half = max(1, batch_size // 2)\n",
    "    for start in range(0, num_pos, half):\n",
    "        end = min(start + half, num_pos)\n",
    "        p = pos_edges[:, perm_pos[start:end]]\n",
    "        n = neg_edges[:, perm_neg[start:end]]  # 与正样本等量\n",
    "\n",
    "        edges = torch.cat([p, n], dim=1)\n",
    "        labels = torch.cat([\n",
    "            torch.ones(p.size(1), device=device),\n",
    "            torch.zeros(n.size(1), device=device)\n",
    "        ])\n",
    "\n",
    "        logits = link_predictor_ogb(node_embeddings_ogb, edges)\n",
    "        loss = criterion_ogb(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_ogb.step()\n",
    "        optimizer_ogb.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss.item() * edges.size(1)\n",
    "        steps += 1\n",
    "\n",
    "    return total_loss / (steps * batch_size)\n",
    "\n",
    "# === 4) 测试函数 ===\n",
    "@torch.no_grad()\n",
    "def test_link_pred_ogb(pos_edge_index, neg_edge_index, batch_size=2048):\n",
    "    link_predictor_ogb.eval()\n",
    "    pos_scores, neg_scores = [], []\n",
    "    P, N = pos_edge_index.size(1), neg_edge_index.size(1)\n",
    "\n",
    "    for s in range(0, P, batch_size):\n",
    "        e = pos_edge_index[:, s:min(s + batch_size, P)]\n",
    "        pos_scores.append(link_predictor_ogb(node_embeddings_ogb, e).sigmoid().cpu())\n",
    "    for s in range(0, N, batch_size):\n",
    "        e = neg_edge_index[:, s:min(s + batch_size, N)]\n",
    "        neg_scores.append(link_predictor_ogb(node_embeddings_ogb, e).sigmoid().cpu())\n",
    "\n",
    "    pos_scores = torch.cat(pos_scores); neg_scores = torch.cat(neg_scores)\n",
    "    scores = torch.cat([pos_scores, neg_scores]).numpy()\n",
    "    labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "# === 5) 训练主循环 ===\n",
    "best_val_auc_ogb, best_state_ogb = 0.0, None\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_link_pred_ogb(batch_size=512)\n",
    "    val_auc = test_link_pred_ogb(split_data_arxiv.val_pos_edge_index, split_data_arxiv.val_neg_edge_index)\n",
    "\n",
    "    if val_auc > best_val_auc_ogb:\n",
    "        best_val_auc_ogb = val_auc\n",
    "        best_state_ogb = {k: v.detach().cpu() for k, v in link_predictor_ogb.state_dict().items()}\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        train_auc = test_link_pred_ogb(split_data_arxiv.train_pos_edge_index, split_data_arxiv.train_neg_edge_index)\n",
    "        test_auc = test_link_pred_ogb(split_data_arxiv.test_pos_edge_index, split_data_arxiv.test_neg_edge_index)\n",
    "        print(f\"[A3-OGB][Epoch {epoch:02d}] loss={loss:.4f} | trainAUC={train_auc:.4f} | \"\n",
    "              f\"valAUC={val_auc:.4f} | testAUC={test_auc:.4f}\")\n",
    "\n",
    "# === 6) 加载最佳模型并输出最终结果 ===\n",
    "if best_state_ogb is not None:\n",
    "    link_predictor_ogb.load_state_dict({k: v.to(device) for k, v in best_state_ogb.items()})\n",
    "\n",
    "final_test_auc_ogb = test_link_pred_ogb(\n",
    "    split_data_arxiv.test_pos_edge_index,\n",
    "    split_data_arxiv.test_neg_edge_index\n",
    ")\n",
    "print(f\"\\n✅ [A3-OGB Ablation] Final Test AUC (OGB-only): {final_test_auc_ogb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9493c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
